{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project on Sentiment Analysis \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis remains one of the key problems that has seen extensive application of natural language processing. This time around, given the tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc, the task is to identify if the tweets have a negative sentiment towards such companies or products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### importing training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:/dataforpython/analytics_vidya_sentiment_analysis/train_2kmZucJ.csv\")\n",
    "test_data = pd.read_csv(\"D:/dataforpython/analytics_vidya_sentiment_analysis/test_oJQbWVk.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data.shape\n",
    "\n",
    "# checking if there are any NA values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking if the data is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"label\"].value_counts()\n",
    "\n",
    "# data is imbalance so we will resample the data\n",
    "# Handling imbalance data (upscaling data)\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = data[data['label'] == 0]\n",
    "df_minority = data[data['label'] == 1]\n",
    "\n",
    "upSample = resample(df_minority, replace=True, n_samples=5894, random_state=0)\n",
    "\n",
    "data = pd.concat([df_majority, upSample])\n",
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the text data from data[\"string\"] column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries for cleaning\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# creating object of porterstemmer, WordNetLemmatizer & stopwords\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# checking length of stop words\n",
    "len(stopwords)\n",
    "\n",
    "# Adding puntuation to set of stopwords\n",
    "punctuation = list(string.punctuation)\n",
    "stopwords.update(punctuation)\n",
    "\n",
    "# checking length after adding puntuation to stopwords\n",
    "len(stopwords)\n",
    "\n",
    "# Resetting index as upscaling will give random numbers.\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "for i in range(0,data.shape[0]):\n",
    "    text = str(data[\"tweet\"][i])\n",
    "    print(i)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords and len(word)>2]\n",
    "    text = \" \".join(text)\n",
    "    print(text)\n",
    "    doc.append(text)\n",
    "\n",
    "# checking shape\n",
    "print(data.shape[0])\n",
    "\n",
    "# converting list to Pandas dataframe\n",
    "data[\"tweet\"] = pd.DataFrame(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Independent & Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer(ngram_range=(1,2),max_features=15000)\n",
    "df = cv.fit_transform(doc).toarray()\n",
    "df.shape\n",
    "\n",
    "# converting to DataFrame for concatinating (optional)\n",
    "x_df = pd.DataFrame(df)\n",
    "\n",
    "# Independent variables\n",
    "x = pd.concat([data.drop([\"tweet\",\"id\",\"label\"],axis=1),x_df],axis=1)\n",
    "x.shape\n",
    "\n",
    "# Target variable\n",
    "y = data[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model with Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x,y)\n",
    "\n",
    "# Performing preprocessing on test data, making copy of the data so that original data dosent get affected\n",
    "test = test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning text for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = []\n",
    "for i in range(0,test.shape[0]):\n",
    "    text = str(test[\"tweet\"][i])\n",
    "    print(i)\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [stemmer.stem(word) for word in text if word not in stopwords and len(word)>2]\n",
    "    text = \" \".join(text)\n",
    "    print(text)\n",
    "    doc1.append(text)\n",
    "\n",
    "# checking shape\n",
    "print(test.shape[0])\n",
    "\n",
    "# converting list to Pandas dataframe\n",
    "test[\"tweet\"] = pd.DataFrame(doc1)\n",
    "\n",
    "# Creating Independent & Target variable\n",
    "df1 = cv.transform(doc1).toarray()\n",
    "df1.shape\n",
    "\n",
    "# converting to DataFrame for concatinating (optional)\n",
    "x_df1 = pd.DataFrame(df1)\n",
    "\n",
    "# Independent variables\n",
    "x_test = pd.concat([test.drop([\"tweet\",\"id\"],axis=1),x_df1],axis=1)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predicting test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(x_test)\n",
    "prediction = pd.DataFrame(y_pred,np.arange(7921,9874))\n",
    "prediction.to_csv(\"D:/dataforpython/analytics_vidya_sentiment_analysis/submit1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
