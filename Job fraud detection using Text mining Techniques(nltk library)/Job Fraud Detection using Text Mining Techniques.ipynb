{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake Job Description Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Data:\n",
    "This dataset contains 18K job descriptions out of which about 800 are fake. The data consists of both textual information and meta-information about the jobs. The dataset can be used to create classification models which can learn the job descriptions which are fraudulent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:/dataforpython/fake_job_postings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data = data.drop([\"job_id\"],axis=1)      # <----- Droping job_id column as it is not required.\n",
    "\n",
    "# Descriptive Statistics \n",
    "data.describe()\n",
    "\n",
    "# Shape of the data\n",
    "data.shape\n",
    "\n",
    "# Info about data\n",
    "data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating copy of data for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA \n",
    "### Fake job postings based on employment type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['required_experience'] = data1['required_experience'].fillna(value='other')\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.countplot(hue='fraudulent',y='required_experience',data=data1)\n",
    "plt.title(\"Fake job postings based on Required Experience\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake job postings based on required experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['required_experience'] = data1['required_experience'].fillna(value='other')\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.countplot(hue='fraudulent',y='required_experience',data=data1)\n",
    "plt.title(\"Fake job postings based on Required Experience\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake job postings based on Required Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['required_education'] = data1['required_education'].fillna(value='other')\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.countplot(hue='fraudulent',y='required_education',data=data1)\n",
    "plt.title(\"Fake job postings based on Required Education\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake job postings based on industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['industry'] = data1['industry'].fillna(value='other')\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.countplot(hue='fraudulent',y= 'industry',data=data1,order=data1.industry.value_counts().iloc[:20].index)\n",
    "plt.title(\"Fake job postings based on Industry\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NA values present in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As salary_range column has 15012 NA records and also we cannot directly assume any salary range,\n",
    "as different companies may have different salary range.\n",
    "Dropping salary_range column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([\"salary_range\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining every column containing strings(text) so that we can apply basic NLP techniques to convert data into int. Filling NA values with blanks i.e \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(\" \",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are any NA values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining String present in each column togethere except columns (\"telecomputing\", \"has_comapny_logo\", \"has_questions\", \"fraudlent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"string\"] = data['title'] + ' ' + data['location'] + ' ' + data['department'] + ' ' + data['company_profile'] + ' ' + data['description'] + ' ' + data['requirements'] + ' ' + data['benefits'] + ' ' + data['employment_type'] + ' ' + data['required_education'] + ' ' + data['industry'] + ' ' + data['function'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample of data[\"string\"], to view if it is done correctly\n",
    "data.string[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After joining all column which contain text we can now drop the columns which we used to join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['title'],axis=1)\n",
    "data = data.drop(['location'],axis=1)\n",
    "data = data.drop(['department'],axis=1)\n",
    "data = data.drop(['company_profile'],axis=1)\n",
    "data = data.drop(['description'],axis=1)\n",
    "data = data.drop(['requirements'],axis=1)\n",
    "data = data.drop(['benefits'],axis=1)\n",
    "data = data.drop(['employment_type'],axis=1)\n",
    "data = data.drop(['required_experience'],axis=1)\n",
    "data = data.drop(['required_education'],axis=1)\n",
    "data = data.drop(['industry'],axis=1)\n",
    "data = data.drop(['function'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if the data is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"fraudulent\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling imbalance data (upscaling data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = data[data['fraudulent'] == 0]\n",
    "df_majority = df_majority.head(6000)\n",
    "df_minority = data[data['fraudulent'] == 1]\n",
    "\n",
    "upSample = resample(df_minority, replace=True, n_samples=6000, random_state=0)\n",
    "\n",
    "data = pd.concat([df_majority, upSample])\n",
    "data['fraudulent'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text data from data[\"string\"] column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries for cleaning\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating object of porterstemmer, WordNetLemmatizer & stopwords\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking length of stop words\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding puntuation to set of stopwords\n",
    "punctuation = list(string.punctuation)\n",
    "stopwords.update(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking length after adding puntuation to stopwords\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting index as upscaling will give random numbers.\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning text\n",
    "doc = []\n",
    "for i in range(0,data.shape[0]):\n",
    "    text = str(data[\"string\"][i])\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in stopwords]\n",
    "    text = \" \".join(text)\n",
    "    print(text)\n",
    "    doc.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking shape\n",
    "print(data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list to Pandas dataframe\n",
    "data[\"string\"] = pd.DataFrame(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud where job_fraudulet = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_job_fraud_0 = data.loc[data[\"fraudulent\"] == 0]\n",
    "doc_job_fraud_0 = doc_job_fraud_0[\"string\"]\n",
    "\n",
    "# For wordcloud we need to convert list into string because list wont work with wordcloud\n",
    "doc_job_fraud_0 = \"\".join(doc_job_fraud_0)\n",
    "\n",
    "# Creating wordcloud will show most frequent occuring words\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(width=1000,height= 500,relative_scaling=1.0,max_words=3500,\n",
    "                      background_color='black',\n",
    "                      stopwords=stopwords,\n",
    "                      min_font_size=10).generate(doc_job_fraud_0)\n",
    "\n",
    "# plot the WordCloud image\n",
    "plt.figure(figsize=(15,10), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()              # After executing this we will get wordcloud of most frequent words used in doc1 (for frequent words we will keep relative scaling =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud where job_fraudulet = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_job_fraud_1 = data.loc[data[\"fraudulent\"] == 1]\n",
    "doc_job_fraud_1 = doc_job_fraud_1[\"string\"]\n",
    "\n",
    "# For wordcloud we need to convert list into string because list wont work with wordcloud\n",
    "doc_job_fraud_1 = \"\".join(doc_job_fraud_1)\n",
    "\n",
    "# Creating wordcloud will show most frequent occuring words\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(width=1000,height= 500,relative_scaling=1.0,max_words=3500,\n",
    "                      background_color='black',\n",
    "                      stopwords=stopwords,\n",
    "                      min_font_size=10).generate(doc_job_fraud_1)\n",
    "\n",
    "# plot the WordCloud image\n",
    "plt.figure(figsize=(15,10), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Independent & Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=30000)\n",
    "df = cv.fit_transform(doc).toarray()\n",
    "df.shape\n",
    "\n",
    "# converting to DataFrame for concatinating (optional)\n",
    "x_df = pd.DataFrame(df)\n",
    "\n",
    "# Independent variables\n",
    "x = pd.concat([data.drop([\"string\",\"fraudulent\"],axis=1),x_df],axis=1)\n",
    "x.shape\n",
    "\n",
    "# Target variable\n",
    "y = data[\"fraudulent\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data into Train and Test (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.30,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model with Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(x_test)\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "confusion_matrix(y_test,y_pred)\n",
    "accuracy_score(y_test,y_pred)\n",
    "print(\"Accuracy with MultinomialNB is \"+str(accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
